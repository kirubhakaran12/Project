---
title: "Project"
author: "Team"
date: "26 September 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r set working directory and read data,,results='asis'}
setwd("C:/Users/kirubha/Desktop/Dataset/Data/Project")
raw_data = read.csv("InsulinPhospho.txt",header = TRUE, sep = "\t")
data <- akt_data <- mTOR_data <- raw_data

akt_substrate = read.csv("Akt_substrates.txt",header = FALSE, sep = "\t")
mTOR_substrate = read.csv("mTOR_substrates.txt",header = FALSE, sep = "\t")

data$Class = ifelse(is.element(data$Identifier,akt_substrate[,1]),1,ifelse(is.element(data$Identifier,mTOR_substrate[,1]),-1,0))

akt_data$Class = ifelse(is.element(akt_data$Identifier,akt_substrate[,1]),1,-1)
mTOR_data$Class = ifelse(is.element(mTOR_data$Identifier,mTOR_substrate[,1]),1,-1)
summary(data)
```

```{r Plot data for visualization}
length(data)

for(i in (4:length(data)-1)){
  
  xval_unlab <- subset(data, Class == 0)[[i]]
  xval_akt <- subset(data, Class == 1)[[i]]
  xval_mTOR <- subset(data, Class == -1)[[i]]
  
  #AUC vs Avg.Fold
  plot(x=xval_unlab,y=subset(data, Class == 0)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
  points(x=xval_akt,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)
  points(x=xval_mTOR,y=subset(data, Class == -1)$Avg.Fold,col="blue",pch=13)
  
}

#AUC vs Avg.Fold
plot(x=subset(data, Class == 0)$AUC,y=subset(data, Class == 0)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)
points(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="blue",pch=13)

#X15s vs Avg.Fold
plot(x=subset(data, Class == 0)$AUC,y=subset(data, Class == 0)$X15s,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$X15s,col="red",pch=13)
points(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$X15s,col="blue",pch=13)

```


```{r Normal distribution}

for(i in 3:12){ 
  rX15 = rnorm(1000,mean(akt_data[,i]),sd(akt_data[,i]))
  drX15 = dnorm(rX15, mean(akt_data[,i]),sd(akt_data[,i]))
  
  mTOR_rX15 = rnorm(1000,mean(mTOR_data[,i]),sd(mTOR_data[,i]))
  mTOR_drX15 = dnorm(mTOR_rX15, mean(mTOR_data[,i]),sd(mTOR_data[,i]))
  
  plot(rX15,drX15,col="red"
       ,ylim =c(min(mTOR_drX15,drX15),max(mTOR_drX15,drX15))
       ,xlim =c(min(mTOR_rX15,rX15),max(mTOR_rX15,rX15))
       ,ylab=colnames(akt_data[i]),
       xlab=paste("Mean_AKT = ",round(mean(akt_data[,i]),2),"Mean_mTOR = ",round(mean(mTOR_data[,i]),2)))
  points(mTOR_rX15,mTOR_drX15,col="blue")
}


#sum(subset(data,Class == 1)$Class)
```
```{r Naive Bayes Classifier}

library(e1071)
nrow(akt_data)
model <- naiveBayes(Class ~ ., data = akt_data[,3:length(akt_data)])
attributes(model)
summary(model)
print(model)
model$tables
?naiveBayes
```

```{r ROSE UPsampling}
library(ROSE)
akt_data.rose <- ROSE(Class~., data=akt_data, seed=3)$data
table(akt_data.rose$Class)

data = akt_data.rose

plot(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)


```

```{r SMOTE UPsampling}
```



```{r SVM}
library(e1071)

svm_data = data[,c(3,4)]
svm.model <- svm(svm_data, y=data$Class, kernel="linear", type="C-classification", scale=FALSE, cost = 0.01)

# coefs: estimated betas
w <- t(svm.model$coefs) %*% svm.model$SV
# rho: the negative intercept of decision boundary
b <- -svm.model$rho

plot(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="red",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="yellow",pch=13)
# plot decision boundary
abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="black", lty=1)
# plot margins
abline(a=(-b-1)/w[1,2], b=-w[1,1]/w[1,2], col="orange", lty=3)
abline(a=(-b+1)/w[1,2], b=-w[1,1]/w[1,2], col="orange", lty=3)
```

```{r Confusion Matrix}

prediction <- predict(svm.model, svm_data) 

tab <- table(pred = prediction, true = data$Class) 
print('contingency table')
tab
```
```{r SVM iterate}
source("C:/Users/kirubha/Desktop/Dataset/functions_w6.R")
library(e1071)


library(caret)
set.seed(1)
fold <- createFolds(data$Class, k=10)

for(deg in c(1:8))
{
  svm.TP <- svm.TN <- svm.FP <- svm.FN <- c()
  for(k in 1:length(fold))
  {
    svm_data = data[fold[[k]],3:16]
    svm.model <- svm(svm_data, y=data[fold[[k]],]$Class, kernel="polynomial", degree=deg, type="C-classification",cost=1)
    prediction <- predict(svm.model, svm_data) 
    
    truth <- data[fold[[k]],]$Class
    
    svm.TP <- c(svm.TP, sum((truth == prediction)[truth == "1"]))
    svm.TN <- c(svm.TN, sum((truth == prediction)[truth == "-1"]))
    svm.FP <- c(svm.FP, sum((truth != prediction)[truth == "-1"]))
    svm.FN <- c(svm.FN, sum((truth != prediction)[truth == "1"]))
  }
  evaluate(svm.TN, svm.FP, svm.TP, svm.FN)
  print("\n")
}
```
```{r LDA iterate}
source("C:/Users/kirubha/Desktop/Dataset/functions_w6.R")
library(e1071)
library(MASS)
library(mlbench)
library(class)
library(caret)

set.seed(5)
fold <- createFolds(data$Class, k=10)

lda.TP <- lda.TN <- lda.FP <- lda.FN <- c()
knn.TP <- knn.TN <- knn.FP <- knn.FN <- c()



for(k in 1:length(fold))
{
  lda.model <- lda(Class~., data=data[-fold[[k]],3:17])
  pred.probs <- predict(lda.model, newdata=data[fold[[k]],3:17])$posterior[,"1"]
  preds <- ifelse(pred.probs > 0.5, "1", "-1")
  
  truth <- data[fold[[k]],]$Class

  lda.TP <- c(lda.TP, sum((truth == preds)[truth == "1"]))
  lda.TN <- c(lda.TN, sum((truth == preds)[truth == "-1"]))
  lda.FP <- c(lda.FP, sum((truth != preds)[truth == "-1"]))
  lda.FN <- c(lda.FN, sum((truth != preds)[truth == "1"]))
  
  preds <- knn(data[-fold[[k]],3:17], data[fold[[k]],3:17], data$Class[-fold[[k]]], k=5)
  knn.TP <- c(knn.TP, sum((truth == preds)[truth == "1"]))
  knn.TN <- c(knn.TN, sum((truth == preds)[truth == "-1"]))
  knn.FP <- c(knn.FP, sum((truth != preds)[truth == "-1"]))
  knn.FN <- c(knn.FN, sum((truth != preds)[truth == "1"]))
}
evaluate(lda.TN, lda.FP, lda.TP, lda.FN)
print("")
evaluate(knn.TN, knn.FP, knn.TP, knn.FN)

  
```
```{r KNN iterate}

```


