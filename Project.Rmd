---
title: "Project"
author: "Team"
date: "26 September 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("C:/Users/kirubha/Desktop/project/R Project/Project/functions_w6.R")
#source("C:/Users/106961/Desktop/R/Data/functions_w6.R")
library(e1071)
library(MASS)
library(mlbench)
library(class)
library(caret)
library(ROSE)
library(sqldf)
library(xlsx)

```


```{r set working directory and read data,,results='asis'}

setwd("C:/Users/kirubha/Desktop/Dataset/Data/Project")
#setwd("C:/Users/106961/Desktop/R/Data")

raw_data = read.csv("InsulinPhospho.txt",header = TRUE, sep = "\t")
data <- akt_data <- mTOR_data <- raw_data

akt_substrate = read.csv("Akt_substrates.txt",header = FALSE, sep = "\t")
mTOR_substrate = read.csv("mTOR_substrates.txt",header = FALSE, sep = "\t")

data$Class = ifelse(is.element(data$Identifier,akt_substrate[,1]),1,ifelse(is.element(data$Identifier,mTOR_substrate[,1]),-1,0))

akt_data$Class = ifelse(is.element(akt_data$Identifier,akt_substrate[,1]),1,-1)
mTOR_data$Class = ifelse(is.element(mTOR_data$Identifier,mTOR_substrate[,1]),1,-1)

pred_2016 = read.csv("Prediction_2016.csv",header = TRUE, sep = ",")

```

```{r ROSE UPsampling}

akt_data.rose <- ROSE(Class~., data=akt_data, seed=3)$data
table(akt_data.rose$Class)

data = akt_data.rose

plot(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)

```

```{r SVM iterate}

svm_model <- function(dat, cl)
{
  for(deg in c(1:3))
  {
    svm.TP <- svm.TN <- svm.FP <- svm.FN <- c()
    for(k in 1:length(fold))
    {
      svm_data <- dat[fold[[k]],]
      svm.model <<- svm(svm_data, y=cl[fold[[k]]], kernel="polynomial", degree=deg, type="C-classification",cost=1,probability = TRUE)
      prediction <- predict(svm.model, svm_data) 
      
      truth <- cl[fold[[k]]]
      
      svm.TP <- c(svm.TP, sum((truth == prediction)[truth == "1"]))
      svm.TN <- c(svm.TN, sum((truth == prediction)[truth == "-1"]))
      svm.FP <- c(svm.FP, sum((truth != prediction)[truth == "-1"]))
      svm.FN <- c(svm.FN, sum((truth != prediction)[truth == "1"]))
    }
    res <- cbind(evaluate(svm.TN, svm.FP, svm.TP, svm.FN),Method = paste("SVM - Degree:",deg),Model_Identifier = mod_cnt, No.Features = ncol(dat), Features = toString(colnames(dat),sep=', '))
    metrics <<- rbind(metrics,res)
    model_fn[[mod_cnt]] <<- svm.model
    mod_cnt <<- mod_cnt + 1
  }
}  

```

```{r SVM Bagging iterate}



svm_model_bag <- function(dat)
{
  positive_class = subset(dat,Class == 1)[,2:ncol(dat)]
  negative_class = subset(dat,Class == -1)[,2:ncol(dat)]

  for(deg in c(3:3))
  {
   
    for(k in 1:50)
    {
      ind <- sample(x=nrow(negative_class),size = nrow(positive_class),replace = FALSE)
      svm_data <- rbind(positive_class,negative_class[ind,])
    
      svm.model <<- svm(svm_data[,1:(ncol(svm_data)-1)], y=svm_data[,ncol(svm_data)], kernel="polynomial", degree=deg, type="C-classification",cost=1,probability = TRUE)
      prediction <- predict(svm.model, dat[,2:(ncol(dat)-1)], probability=TRUE) 
      svm_pred_Prob <- cbind(akt_data[1],prob=attr(prediction, "probabilities")[,1])
      prob_pred <<- rbind(prob_pred, svm_pred_Prob)
    }
  }
}  

```

```{r LDA iterate}

lda_model <- function(dat,cl)
{
  lda.TP <- lda.TN <- lda.FP <- lda.FN <- c()
  dat = cbind(dat,Class=cl)
  for(k in 1:length(fold))
  {
    lda.model <- lda(Class~., data=dat[-fold[[k]],])
    pred.probs <- predict(lda.model, newdata=dat[fold[[k]],])$posterior[,"1"]
    preds <- ifelse(pred.probs > 0.5, "1", "-1")
    
    truth <- dat[fold[[k]],]$Class
  
    lda.TP <- c(lda.TP, sum((truth == preds)[truth == "1"]))
    lda.TN <- c(lda.TN, sum((truth == preds)[truth == "-1"]))
    lda.FP <- c(lda.FP, sum((truth != preds)[truth == "-1"]))
    lda.FN <- c(lda.FN, sum((truth != preds)[truth == "1"]))
  }
  res <- cbind(evaluate(lda.TN, lda.FP, lda.TP, lda.FN),Method = paste("LDA"),Model_Identifier = mod_cnt, No.Features = ncol(dat), Features = toString(colnames(dat),sep=', '))
  metrics <<- rbind(metrics,res)
  model_fn[[mod_cnt]] <<- lda.model
  mod_cnt <<- mod_cnt + 1
}

```

```{r KNN iterate}

knn_model <- function(dat,cl)
{
  knn.TP <- knn.TN <- knn.FP <- knn.FN <- c()
  dat = cbind(dat,Class=cl)
  
  for(n in c(1,10,20,50,100))
  {
    for(k in 1:length(fold))
    {
      truth <- dat[fold[[k]],]$Class
      preds <- knn(dat[-fold[[k]],], dat[fold[[k]],], dat$Class[-fold[[k]]], k=n)
      knn.TP <- c(knn.TP, sum((truth == preds)[truth == "1"]))
      knn.TN <- c(knn.TN, sum((truth == preds)[truth == "-1"]))
      knn.FP <- c(knn.FP, sum((truth != preds)[truth == "-1"]))
      knn.FN <- c(knn.FN, sum((truth != preds)[truth == "1"]))
    }
    res <- cbind(evaluate(knn.TN, knn.FP, knn.TP, knn.FN),Method = paste("KNN, Nearest neighbour:",n),Model_Identifier = mod_cnt, No.Features = ncol(dat), Features = toString(colnames(dat),sep=', '))
    metrics <<- rbind(metrics,res)
    model_fn[[mod_cnt]] <<- preds
    mod_cnt <<- mod_cnt + 1
  }
}

```

```{r Feature Prioratization }

feature.pvalues <- c()
for(i in 3:16) 
{
  feature.pvalues <- c(feature.pvalues, t.test(subset(data, Class == 1)[[i]], subset(data, Class == -1)[[i]])$p.value)
}
names(feature.pvalues) <- colnames(data[,3:16])
filtered.features2 <- names(sort(feature.pvalues))

```

```{r Model Selection}

set.seed(1)
fold <- createFolds(data$Class, k=10)

metrics <<- data.frame()
mod_cnt <<- 1
model_fn <<- c()

model <- function(dat,cl)
{
  svm_model(dat,cl)
  lda_model(dat,cl)
  knn_model(dat,cl)
}


for(i in c(1:length(filtered.features2)))
{
  idn <- data[1]
  dat <- data.frame(data[,filtered.features2[1:i]])
  cl <- data$Class
  model(dat,cl)
}

metrics

metrics[grep("SVM", metrics$Method),]

##bagging

prob_pred <- c()
svm_pred_Prob <- c()
svm_model_bag(akt_data[,c(1,3:(3+i))])
svm_bag_prob <- sqldf('SELECT Identifier, AVG(prob) AS B FROM prob_pred GROUP BY Identifier')

#Use Model

BIC(use_model)
use_model <- model_fn[[3]]
prediction <- predict(use_model, dat,probability = TRUE) 
new_prob <- cbind(idn,dat,(attr(prediction, "probabilities")))

new_prob[order(new_prob$"1"),]

new_prob()

```

```{r Model comparison}

old_prediction <- data.frame(Identifier = paste(toupper(pred_2016[,1]),";",pred_2016[,2],";",sep=""),pred_2016[,c("Full.model.predict","Motif.predict","Phosphoproteome.predict")])
svm_bag_prob

output <- merge(x = old_prediction, y = svm_bag_prob, by = "Identifier", all = TRUE)
write.xlsx(output, "C:/Users/106961/Desktop/R/Data/output.xlsx")

write.csv(output, "C:/Users/kirubha/Desktop/Dataset/Data/Project/output.csv")


```



```{r SVM}
library(e1071)

svm_data = data[,c(3,4)]
svm.model <- svm(svm_data, y=data$Class, kernel="linear", type="C-classification", scale=FALSE, cost = 0.01)

# coefs: estimated betas
w <- t(svm.model$coefs) %*% svm.model$SV
# rho: the negative intercept of decision boundary
b <- -svm.model$rho

plot(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="red",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="yellow",pch=13)
# plot decision boundary
abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="black", lty=1)
# plot margins
abline(a=(-b-1)/w[1,2], b=-w[1,1]/w[1,2], col="orange", lty=3)
abline(a=(-b+1)/w[1,2], b=-w[1,1]/w[1,2], col="orange", lty=3)
```

```{r Confusion Matrix}

prediction <- predict(svm.model, svm_data) 

tab <- table(pred = prediction, true = data$Class) 
print('contingency table')
tab
```

```{r Naive Bayes Classifier}

library(e1071)
nrow(akt_data)
model <- naiveBayes(Class ~ ., data = akt_data[,3:length(akt_data)])
attributes(model)
summary(model)
print(model)
model$tables
?naiveBayes
```


```{r Plot data for visualization}
length(data)

for(i in (4:length(data)-1)){
  
  xval_unlab <- subset(data, Class == 0)[[i]]
  xval_akt <- subset(data, Class == 1)[[i]]
  xval_mTOR <- subset(data, Class == -1)[[i]]
  
  #AUC vs Avg.Fold
  plot(x=xval_unlab,y=subset(data, Class == 0)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
  points(x=xval_akt,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)
  points(x=xval_mTOR,y=subset(data, Class == -1)$Avg.Fold,col="blue",pch=13)
  
}

#AUC vs Avg.Fold
plot(x=subset(data, Class == 0)$AUC,y=subset(data, Class == 0)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)
points(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="blue",pch=13)

#X15s vs Avg.Fold
plot(x=subset(data, Class == 0)$AUC,y=subset(data, Class == 0)$X15s,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$X15s,col="red",pch=13)
points(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$X15s,col="blue",pch=13)

```

```{r Normal distribution}

for(i in 3:12){ 
  rX15 = rnorm(1000,mean(akt_data[,i]),sd(akt_data[,i]))
  drX15 = dnorm(rX15, mean(akt_data[,i]),sd(akt_data[,i]))
  
  mTOR_rX15 = rnorm(1000,mean(mTOR_data[,i]),sd(mTOR_data[,i]))
  mTOR_drX15 = dnorm(mTOR_rX15, mean(mTOR_data[,i]),sd(mTOR_data[,i]))
  
  plot(rX15,drX15,col="red"
       ,ylim =c(min(mTOR_drX15,drX15),max(mTOR_drX15,drX15))
       ,xlim =c(min(mTOR_rX15,rX15),max(mTOR_rX15,rX15))
       ,ylab=colnames(akt_data[i]),
       xlab=paste("Mean_AKT = ",round(mean(akt_data[,i]),2),"Mean_mTOR = ",round(mean(mTOR_data[,i]),2)))
  points(mTOR_rX15,mTOR_drX15,col="blue")
}


#sum(subset(data,Class == 1)$Class)
```