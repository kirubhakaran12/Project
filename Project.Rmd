---
title: "Project"
author: "Team"
date: "26 September 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#source("C:/Users/kirubha/Desktop/Dataset/functions_w6.R")
source("C:/Users/106961/Desktop/R/Data/functions_w6.R")
library(e1071)
library(MASS)
library(mlbench)
library(class)
library(caret)
library(ROSE)

```


```{r set working directory and read data,,results='asis'}

#setwd("C:/Users/kirubha/Desktop/Dataset/Data/Project")
setwd("C:/Users/106961/Desktop/R/Data")

raw_data = read.csv("InsulinPhospho.txt",header = TRUE, sep = "\t")
data <- akt_data <- mTOR_data <- raw_data

akt_substrate = read.csv("Akt_substrates.txt",header = FALSE, sep = "\t")
mTOR_substrate = read.csv("mTOR_substrates.txt",header = FALSE, sep = "\t")

data$Class = ifelse(is.element(data$Identifier,akt_substrate[,1]),1,ifelse(is.element(data$Identifier,mTOR_substrate[,1]),-1,0))

akt_data$Class = ifelse(is.element(akt_data$Identifier,akt_substrate[,1]),1,-1)
mTOR_data$Class = ifelse(is.element(mTOR_data$Identifier,mTOR_substrate[,1]),1,-1)

```

```{r ROSE UPsampling}

akt_data.rose <- ROSE(Class~., data=akt_data, seed=3)$data
table(akt_data.rose$Class)

data = akt_data.rose

plot(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)

```

```{r SVM iterate}

svm_model <- function(dat, cl)
{
  for(deg in c(1:10))
  {
    svm.TP <- svm.TN <- svm.FP <- svm.FN <- c()
    for(k in 1:length(fold))
    {
      svm_data <- dat[fold[[k]],]
      svm.model <<- svm(svm_data, y=cl[fold[[k]]], kernel="polynomial", degree=deg, type="C-classification",cost=1)
      prediction <- predict(svm.model, svm_data) 
      
      truth <- cl[fold[[k]]]
      
      svm.TP <- c(svm.TP, sum((truth == prediction)[truth == "1"]))
      svm.TN <- c(svm.TN, sum((truth == prediction)[truth == "-1"]))
      svm.FP <- c(svm.FP, sum((truth != prediction)[truth == "-1"]))
      svm.FN <- c(svm.FN, sum((truth != prediction)[truth == "1"]))
    }
    res <- cbind(evaluate(svm.TN, svm.FP, svm.TP, svm.FN),Method = paste("SVM - Degree:",deg))
    metrics <<- rbind(metrics,res)
  }
}  

```

```{r LDA iterate}

lda_model <- function(dat,cl)
{
  lda.TP <- lda.TN <- lda.FP <- lda.FN <- c()
  dat = cbind(dat,Class=cl)
  for(k in 1:length(fold))
  {
    lda.model <- lda(Class~., data=dat[-fold[[k]],])
    pred.probs <- predict(lda.model, newdata=dat[fold[[k]],])$posterior[,"1"]
    preds <- ifelse(pred.probs > 0.5, "1", "-1")
    
    truth <- dat[fold[[k]],]$Class
  
    lda.TP <- c(lda.TP, sum((truth == preds)[truth == "1"]))
    lda.TN <- c(lda.TN, sum((truth == preds)[truth == "-1"]))
    lda.FP <- c(lda.FP, sum((truth != preds)[truth == "-1"]))
    lda.FN <- c(lda.FN, sum((truth != preds)[truth == "1"]))
  }
  res <- cbind(evaluate(lda.TN, lda.FP, lda.TP, lda.FN),Method = paste("LDA"))
  metrics <<- rbind(metrics,res)
}

```

```{r KNN iterate}

knn_model <- function(dat,cl)
{
  knn.TP <- knn.TN <- knn.FP <- knn.FN <- c()
  dat = cbind(dat,Class=cl)
  
  for(n in c(1:10))
  {
    for(k in 1:length(fold))
    {
      truth <- dat[fold[[k]],]$Class
      preds <- knn(dat[-fold[[k]],], dat[fold[[k]],], dat$Class[-fold[[k]]], k=n)
      knn.TP <- c(knn.TP, sum((truth == preds)[truth == "1"]))
      knn.TN <- c(knn.TN, sum((truth == preds)[truth == "-1"]))
      knn.FP <- c(knn.FP, sum((truth != preds)[truth == "-1"]))
      knn.FN <- c(knn.FN, sum((truth != preds)[truth == "1"]))
    }
    res <- cbind(evaluate(knn.TN, knn.FP, knn.TP, knn.FN),Method = paste("KNN, Nearest neighbour:",n))
    metrics <<- rbind(metrics,res)
  }
}

```

```{r Model Selection}

set.seed(1)
fold <- createFolds(data$Class, k=10)


model <- function(dat,cl)
{
  metrics <<- data.frame()
  svm_model(dat,cl)
  lda_model(dat,cl)
  knn_model(dat,cl)
}

#Model 1

dat <- data[,3:16]
cl <- data$Class

model(dat,cl)


##End of Model 1

#Model 2

dat <- data[,3:12]
cl <- data$Class

model(dat,cl)

##End of Model 2

metrics
```




```{r SVM}
library(e1071)

svm_data = data[,c(3,4)]
svm.model <- svm(svm_data, y=data$Class, kernel="linear", type="C-classification", scale=FALSE, cost = 0.01)

# coefs: estimated betas
w <- t(svm.model$coefs) %*% svm.model$SV
# rho: the negative intercept of decision boundary
b <- -svm.model$rho

plot(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="red",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="yellow",pch=13)
# plot decision boundary
abline(a=-b/w[1,2], b=-w[1,1]/w[1,2], col="black", lty=1)
# plot margins
abline(a=(-b-1)/w[1,2], b=-w[1,1]/w[1,2], col="orange", lty=3)
abline(a=(-b+1)/w[1,2], b=-w[1,1]/w[1,2], col="orange", lty=3)
```

```{r Confusion Matrix}

prediction <- predict(svm.model, svm_data) 

tab <- table(pred = prediction, true = data$Class) 
print('contingency table')
tab
```

```{r Naive Bayes Classifier}

library(e1071)
nrow(akt_data)
model <- naiveBayes(Class ~ ., data = akt_data[,3:length(akt_data)])
attributes(model)
summary(model)
print(model)
model$tables
?naiveBayes
```


```{r Plot data for visualization}
length(data)

for(i in (4:length(data)-1)){
  
  xval_unlab <- subset(data, Class == 0)[[i]]
  xval_akt <- subset(data, Class == 1)[[i]]
  xval_mTOR <- subset(data, Class == -1)[[i]]
  
  #AUC vs Avg.Fold
  plot(x=xval_unlab,y=subset(data, Class == 0)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
  points(x=xval_akt,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)
  points(x=xval_mTOR,y=subset(data, Class == -1)$Avg.Fold,col="blue",pch=13)
  
}

#AUC vs Avg.Fold
plot(x=subset(data, Class == 0)$AUC,y=subset(data, Class == 0)$Avg.Fold,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$Avg.Fold,col="red",pch=13)
points(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$Avg.Fold,col="blue",pch=13)

#X15s vs Avg.Fold
plot(x=subset(data, Class == 0)$AUC,y=subset(data, Class == 0)$X15s,col="yellow",pch=13,xlim=c(0,1),ylim=c(-1,5))
points(x=subset(data, Class == 1)$AUC,y=subset(data, Class == 1)$X15s,col="red",pch=13)
points(x=subset(data, Class == -1)$AUC,y=subset(data, Class == -1)$X15s,col="blue",pch=13)

```

```{r Normal distribution}

for(i in 3:12){ 
  rX15 = rnorm(1000,mean(akt_data[,i]),sd(akt_data[,i]))
  drX15 = dnorm(rX15, mean(akt_data[,i]),sd(akt_data[,i]))
  
  mTOR_rX15 = rnorm(1000,mean(mTOR_data[,i]),sd(mTOR_data[,i]))
  mTOR_drX15 = dnorm(mTOR_rX15, mean(mTOR_data[,i]),sd(mTOR_data[,i]))
  
  plot(rX15,drX15,col="red"
       ,ylim =c(min(mTOR_drX15,drX15),max(mTOR_drX15,drX15))
       ,xlim =c(min(mTOR_rX15,rX15),max(mTOR_rX15,rX15))
       ,ylab=colnames(akt_data[i]),
       xlab=paste("Mean_AKT = ",round(mean(akt_data[,i]),2),"Mean_mTOR = ",round(mean(mTOR_data[,i]),2)))
  points(mTOR_rX15,mTOR_drX15,col="blue")
}


#sum(subset(data,Class == 1)$Class)
```